{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Solutions for GirlsWhoML Data Interpretability Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and choose layer (preliminary)\n",
    "import numpy as np\n",
    "\n",
    "# 1) Download the .npz file from GitHub into your Colab environment.\n",
    "!wget https://github.com/aideenfay/girlswhoml/blob/main/clean_reduced.npz?raw=true -O clean_reduced.npz\n",
    "!wget https://github.com/aideenfay/girlswhoml/blob/main/poisoned_reduced.npz?raw=true -O poisoned_reduced.npz\n",
    "\n",
    "clean_data = np.load(\"clean_reduced.npz\")['arr_0']\n",
    "poisoned_data = np.load(\"poisoned_reduced.npz\")['arr_0']\n",
    "\n",
    "# 2) Choose a layer to interpret\n",
    "layer = 15\n",
    "X_clean = clean_data[:, layer, :]   \n",
    "X_poisoned = poisoned_data[:, layer, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine datasets and create labels\n",
    "X = np.vstack([X_clean, X_poison])\n",
    "y = np.concatenate([np.zeros(len(X_clean)), np.ones(len(X_poison))])\n",
    "\n",
    "# 2. Apply PCA\n",
    "pca = PCA(n_components=2)  # TODO: choose number of principal components (e.g., 2 for visualization)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# 3. Check explained variance\n",
    "print(\"Explained variance ratio of each component:\", pca.explained_variance_ratio_)\n",
    "print(\"Cumulative explained variance:\", np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "# 4. Visualize the first two principal components\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='coolwarm', alpha=0.7)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA projection (2 components)')\n",
    "plt.colorbar(ticks=[0,1], label='Poisoned (0=clean, 1=poisoned)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. t-SNE\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 1. Define t-SNE model\n",
    "tsne = TSNE(n_components=2, perplexity=30 random_state=42)   \n",
    "\n",
    "# 2. Fit-transform the data using t-SNE (this may take a few seconds)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# 3. Plot t-SNE results\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='coolwarm', alpha=0.7)\n",
    "plt.title(f\"t-SNE projection (perplexity={tsne.perplexity})\")\n",
    "plt.xlabel('t-SNE dimension 1')\n",
    "plt.ylabel('t-SNE dimension 2')\n",
    "plt.colorbar(ticks=[0,1], label='Poisoned (0=clean, 1=poisoned)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Persistent Homology and TDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The working code is provided in the student version of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Logistic Regression Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Create combined dataset X and label vector y\n",
    "# ----------------------------\n",
    "X = np.vstack([X_clean, X_poison])  # shape (400, 4096) if each subset is (200, 4096)\n",
    "y = np.concatenate([\n",
    "    np.zeros(len(X_clean)),    # 0 = clean\n",
    "    np.ones(len(X_poison))     # 1 = poisoned\n",
    "])\n",
    "\n",
    "print(\"Combined data shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)\n",
    "print(\"Number of clean samples:\", np.sum(y == 0))\n",
    "print(\"Number of poisoned samples:\", np.sum(y == 1))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Split into train/test sets\n",
    "# ----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "print(\"Training set size:\", X_train.shape[0], \"Test set size:\", X_test.shape[0])\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Train logistic regression\n",
    "# ----------------------------\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Evaluate on test set\n",
    "# ----------------------------\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Calculate ROCâ€“AUC and plot\n",
    "# ----------------------------\n",
    "# 5a) Predict probability for positive class (poisoned=1)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]  # Probability of class=1\n",
    "\n",
    "# 5b) Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"ROC AUC Score: {auc_score:.3f}\")\n",
    "\n",
    "# 5c) Plot the ROC curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, color='blue', label=f\"ROC curve (AUC = {auc_score:.3f})\")\n",
    "plt.plot([0,1], [0,1], color='gray', linestyle='--', label=\"Random Guessing\")\n",
    "\n",
    "plt.title(\"ROC Curve - Logistic Regression\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Graph Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy.linalg as la\n",
    "\n",
    "# For a heatmap, we'll use seaborn  \n",
    "!pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "!pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "###############################################################################\n",
    "# 1) Larger set of ~30 sentences across sports, tech, and cooking\n",
    "###############################################################################\n",
    "sentences = [\n",
    "    # Sports domain\n",
    "    \"The soccer match ended in a dramatic penalty shootout.\",\n",
    "    \"Basketball players often exhibit impressive vertical jumps and agility.\",\n",
    "    \"She set a new track record in the 100-meter dash.\",\n",
    "    \"The tennis tournament drew huge crowds all week.\",\n",
    "    \"He scored a touchdown on the final drive of the game.\",\n",
    "    \"A well-placed corner kick led to the winning goal.\",\n",
    "    \"Rugby is a physically demanding sport that requires endurance.\",\n",
    "    \"Mountain biking trails can vary greatly in difficulty.\",\n",
    "    \"The swimming relay team broke the national record.\",\n",
    "    \"A marathon requires careful pacing and hydration strategies.\",\n",
    "\n",
    "    # Tech domain\n",
    "    \"Cloud computing services offer flexible resource scaling.\",\n",
    "    \"Quantum computing promises exponential speedups for specific tasks.\",\n",
    "    \"Machine learning algorithms thrive on large volumes of data.\",\n",
    "    \"Cybersecurity threats evolve rapidly in modern networks.\",\n",
    "    \"Neural networks draw inspiration from the human brain.\",\n",
    "    \"Blockchain technology underpins many cryptocurrencies.\",\n",
    "    \"Augmented reality applications are gaining popularity in gaming.\",\n",
    "    \"Autonomous vehicles rely on sophisticated sensor fusion.\",\n",
    "    \"The microchip shortage has impacted global supply chains.\",\n",
    "    \"Robotics advancements continuously push the boundaries of automation.\",\n",
    "\n",
    "    # Cooking domain\n",
    "    \"Baking sourdough bread requires patience and a good starter.\",\n",
    "    \"A slow cooker can simplify meal preparation for busy families.\",\n",
    "    \"He garnished the salad with toasted almonds and feta cheese.\",\n",
    "    \"Gluten-free baking often demands special flour blends.\",\n",
    "    \"Homemade pasta sauce benefits from a long simmer and fresh herbs.\",\n",
    "    \"Marinating the chicken overnight enhances flavor.\",\n",
    "    \"She whipped up a batch of chocolate chip cookies from scratch.\",\n",
    "    \"Roasted vegetables drizzled with olive oil can be quite addictive.\",\n",
    "    \"The casserole combined layers of cheese, meat, and tomato sauce.\",\n",
    "    \"Pancakes made with buttermilk tend to be fluffier.\"\n",
    "]\n",
    "\n",
    "# Print them out for reference\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"[{i}] {s}\")\n",
    "print(f\"\\nTotal sentences: {len(sentences)}\")\n",
    "\n",
    "# Assign domain labels for color-coding:\n",
    "# We'll do 0 = Sports, 1 = Tech, 2 = Cooking.\n",
    "domain_labels = (\n",
    "    [0]*10  +  # first 10 are sports\n",
    "    [1]*10  +  # next 10 are tech\n",
    "    [2]*10     # last 10 are cooking\n",
    ")\n",
    "domain_names = {0: \"Sports\", 1: \"Tech\", 2: \"Cooking\"}\n",
    "\n",
    "###############################################################################\n",
    "# 2) Obtain LLM embeddings\n",
    "###############################################################################\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedder = SentenceTransformer(model_name)\n",
    "X_emb = embedder.encode(sentences, show_progress_bar=False)\n",
    "X_emb = np.array(X_emb)\n",
    "print(\"Embeddings shape:\", X_emb.shape)\n",
    "\n",
    "###############################################################################\n",
    "# 3) Build a k-NN graph in embedding space\n",
    "###############################################################################\n",
    "def build_knn_graph(X, k=5):\n",
    "    \"\"\"\n",
    "    Constructs a k-NN adjacency matrix (binary, symmetrical).\n",
    "    Adjust 'k' based on dataset size. \n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='euclidean')\n",
    "    nbrs.fit(X)\n",
    "    _, indices = nbrs.kneighbors(X)\n",
    "\n",
    "    A = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j_idx in indices[i]:\n",
    "            A[i, j_idx] = 1\n",
    "            A[j_idx, i] = 1  # symmetrize\n",
    "    return A\n",
    "\n",
    "k_neighbors = 5\n",
    "A = build_knn_graph(X_emb, k=k_neighbors)\n",
    "print(\"Adjacency matrix shape:\", A.shape)\n",
    "\n",
    "###############################################################################\n",
    "# 3a) Visualize adjacency matrix as a heatmap\n",
    "###############################################################################\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(A, cmap='Greens', annot=False, xticklabels=False, yticklabels=False)\n",
    "plt.title(f\"Adjacency Matrix Heatmap (k={k_neighbors} NN)\")\n",
    "plt.show()\n",
    "\n",
    "# If you want row/col labels, you can pass xticklabels=sentences or domain_labels,\n",
    "# but it can be cluttered with 30 sentences.\n",
    "\n",
    "###############################################################################\n",
    "# 4) Compute the graph Laplacian L = D - A\n",
    "###############################################################################\n",
    "degrees = A.sum(axis=1)\n",
    "D = np.diag(degrees)\n",
    "L = D - A\n",
    "\n",
    "# Eigen-decomposition\n",
    "eigvals, eigvecs = la.eigh(L)\n",
    "idx = np.argsort(eigvals)\n",
    "eigvals = eigvals[idx]\n",
    "eigvecs = eigvecs[:, idx]\n",
    "\n",
    "print(\"Eigenvalues (sorted):\", eigvals)\n",
    "print(\"\\nFirst 10 eigenvalues:\", eigvals[:10])\n",
    "\n",
    "# Plot the eigenvalue spectrum\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(eigvals, marker='o')\n",
    "plt.title(f\"Laplacian Eigenvalues (k={k_neighbors} NN)\")\n",
    "plt.xlabel(\"Eigenvalue Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# 5) Fiedler Vector Partition\n",
    "###############################################################################\n",
    "fiedler_index = 1  # second smallest eigenvalue\n",
    "if fiedler_index < len(eigvals):\n",
    "    fiedler_vector = eigvecs[:, fiedler_index]\n",
    "    groupA = np.where(fiedler_vector >= 0)[0]\n",
    "    groupB = np.where(fiedler_vector < 0)[0]\n",
    "\n",
    "    print(\"\\n=== Partition by Fiedler Vector Sign ===\")\n",
    "    print(\"Group A indices:\", groupA)\n",
    "    print(\"Group B indices:\", groupB)\n",
    "\n",
    "    print(\"\\nGroup A Sentences:\")\n",
    "    for idx in groupA:\n",
    "        print(f\"  - [{idx}] ({domain_names[domain_labels[idx]]}) {sentences[idx]}\")\n",
    "\n",
    "    print(\"\\nGroup B Sentences:\")\n",
    "    for idx in groupB:\n",
    "        print(f\"  - [{idx}] ({domain_names[domain_labels[idx]]}) {sentences[idx]}\")\n",
    "else:\n",
    "    print(\"Not enough eigenvalues to do a Fiedler partition. Check adjacency or data size.\")\n",
    "\n",
    "###############################################################################\n",
    "# 6) 2D Spectral Embedding (Eigenvectors 2 & 3)\n",
    "###############################################################################\n",
    "if len(eigvals) > 2:\n",
    "    x_coords = eigvecs[:, 1]  # second eigenvector\n",
    "    y_coords = eigvecs[:, 2]  # third eigenvector\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    # We'll color each point by domain_labels, so we can see if it separates sports/tech/cooking\n",
    "    scatter = plt.scatter(x_coords, y_coords, c=domain_labels, cmap='Accent', alpha=0.8)\n",
    "\n",
    "    # Annotate points by index so we can reference them\n",
    "    for i in range(len(sentences)):\n",
    "        plt.annotate(str(i), (x_coords[i]+0.01, y_coords[i]+0.01))\n",
    "\n",
    "    cbar = plt.colorbar(scatter, ticks=[0,1,2])\n",
    "    cbar.ax.set_yticklabels(['Sports', 'Tech', 'Cooking'])\n",
    "    plt.title(\"Spectral Embedding (Eigenvectors 2 & 3)\\nColored by Domain Label\")\n",
    "    plt.xlabel(\"2nd Eigenvector\")\n",
    "    plt.ylabel(\"3rd Eigenvector\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough eigenvectors to create a 2D spectral embedding.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
